\documentclass[11pt,a4paper]{article}
\usepackage{lmodern}

\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
    \usepackage{xltxtra,xunicode}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Mapping=tex-text,Scale=MatchLowercase}
  \newcommand{\euro}{???}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[lmargin=2.5cm,rmargin=2.5cm,tmargin=2.5cm,bmargin=2.5cm]{geometry}

% Figure Placement:
\usepackage{float}
\let\origfigure\figure
\let\endorigfigure\endfigure
\renewenvironment{figure}[1][2] {
    \expandafter\origfigure\expandafter[H]
} {
    \endorigfigure
}

%% citation setup
\usepackage{csquotes}

\usepackage[backend=biber, maxbibnames = 99, style = apa]{biblatex}
\setlength\bibitemsep{1.5\itemsep}
\addbibresource{R_packages.bib}
\bibliography{ref.bib}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\ifxetex
  \usepackage[setpagesize=false, % page size defined by xetex
              unicode=false, % unicode breaks when used with xetex
              xetex]{hyperref}
\else
  \usepackage[unicode=true, linktocpage = TRUE]{hyperref}
\fi
\hypersetup{breaklinks=true,
            bookmarks=true,
            pdfauthor={Alex Amaguaya, Lea Bergmann},
            pdftitle={The Gender Pay Gap in the General Social Survey},
            colorlinks=true,
            citecolor=blue,
            urlcolor=blue,
            linkcolor=magenta,
            pdfborder={0 0 0}}
\urlstyle{same}  % don't use monospace font for urls
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\setcounter{secnumdepth}{5}

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\newcommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}
  \title{The Gender Pay Gap in the General Social Survey}
  \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
\subtitle{Statistical Learning}
  \author{Alex Amaguaya, Lea Bergmann}
  \preauthor{\centering\large\emph}
  \postauthor{\par}
  \predate{\centering\large\emph}
  \postdate{\par}
  \date{today}


%% linespread settings

\usepackage{setspace}

\onehalfspacing

% Language Setup

\usepackage{ifthen}
\usepackage{iflang}
\usepackage[super]{nth}
\usepackage[ngerman, english]{babel}

%Acronyms
\usepackage[printonlyused, withpage, nohyperlinks]{acronym}
\usepackage{changepage}

% Multicols for the Title page
\usepackage{multicol}


\usepackage{longtable}

\begin{document}

\selectlanguage{english}


%\maketitle

\begin{titlepage}
  \noindent\begin{minipage}{0.6\textwidth}
	  \IfLanguageName{english}{University of Duisburg-Essen}{Universit\"at Duisburg-Essen}\\
	  \IfLanguageName{english}{Faculty of Business Administration and Economics}{Fakult\"at f\"ur Wirtschaftswissensschaften}\\
	  \IfLanguageName{english}{Chair of Econometrics}{Lehrstuhl f\"ur \"Okonometrie}\\
  \end{minipage}
	\begin{minipage}{0.4\textwidth}
	  \begin{flushright}
  	  \vspace{-0.5cm}
      \IfLanguageName{english}{\includegraphics*[width=5cm]{Includes/duelogo_en.png}}{\includegraphics*[width=5cm]{Includes/duelogo_de.png}}
	  \end{flushright}
	\end{minipage}
  \\
  \vspace{1.5cm}
  \begin{center}
  \huge{The Gender Pay Gap in the General Social Survey}\\
  \vspace{.25cm}
  \Large{Statistical Learning}\\
  \vspace{0.5cm}
  \large{Term Paper}\\
  \vspace{1cm}
  \large{
  \IfLanguageName{english}{Submitted to the Faculty of \\ Business Administration and Economics \\at the \\University of Duisburg-Essen}{Vorgelegt der \\Fakult\"at f\"ur Wirtschaftswissenschaften der \\ Universit\"at Duisburg-Essen}\\}
  \vspace{0.75cm}
  \large{\IfLanguageName{english}{from:}{von:}}\\
  \vspace{0.5cm}
  Alex Amaguaya, Lea Bergmann\\
  \end{center}
  %\vspace{2cm}
  \vfill
  \hrulefill

  \noindent\begin{minipage}[t]{0.3\textwidth}
  \IfLanguageName{english}{Reviewer:}{Erstgutachter:}
  \end{minipage}
  \begin{minipage}[t]{0.7\textwidth}
  \hspace{1cm}
  \end{minipage}

  \noindent\begin{minipage}[t]{0.3\textwidth}
  \IfLanguageName{english}{Deadline:}{Abgabefrist:}
  \end{minipage}
  \begin{minipage}[t]{0.7\textwidth}
  \hspace{1cm}tomorrow
  \end{minipage}

  \hrulefill

  \begin{multicols}{3}

  Name:

  Matriculation No.:

  E-Mail:

  Study Path:

  Semester:

  Graduation (est.):

  \columnbreak

 Lea Bergmann

  123456

lea.bergmann@rwi-essen.de
lea.bergmann.1j@stud.uni-due.de

PhD Economics

  \nth{1}

  Summer Term 2023

  \columnbreak

  John Doe

  234567

  john.doe@web.de

  M.Sc. Economics

  \nth{4}

  Summer Term 2020


  \end{multicols}

\end{titlepage}

\newgeometry{top=2cm, left = 5cm, right = 2.5cm, bottom = 2.5cm}


\pagenumbering{Roman}
{
\hypersetup{linkcolor=black}

\setcounter{tocdepth}{3}
\tableofcontents
}

\newpage
\listoffigures
\addcontentsline{toc}{section}{List of Figures}

%\newpage
\listoftables
\addcontentsline{toc}{section}{List of Tables}

\section*{List of Abbreviations}
\addcontentsline{toc}{section}{List of Abbreviations}

\begin{adjustwidth}{1.5em}{0pt}

\begin{acronym}[dummyyyy]
 \acro{ECTSCP}{European Credit Transfer System Credit Point}
 \acro{lasso}{Least Absolute Shrinkage and Selection Operator}
 \acro{pcr}{Principal Components Regression}
 \acro{pls}{Partial Least Squares}
 \acro{RMSE}{Root Mean Squared Error}
 \acroplural{LRG}[LRG]{laengefristige Refinanzierungsgeschaefte}

%Falls eine Abkuerzung in der Mehrzahl nicht einfach auf "s" endet muss das speziell eingestellt werden.
% \acro{slmtA}{super lange mega tolle Abkuerzung} %Einzahl
 %\acroplural{slmtA}[slmtAs]{super lange mega tolle Abkuerzungen} %Mehrzahl
 \acro{dummyyyy}{dummyyy}
\end{acronym}

\end{adjustwidth}

\restoregeometry

\newpage
\pagenumbering{arabic}
\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

In this paper we predict individuals' wages using data from the General
Social Survey (GSS, 1974-2018). The variable of interest is
\texttt{realrinc} which varies between 227\$ and
480,145\$\footnote{In constant 1986 USD.}. Hence, we can use regression
techniques to predict individuals' wages. Additionally, we can consider
whether \texttt{realrinc} varies by gender and if so, how much.

The data from the GSS allow us to consider various control variables
such as gender, age, education, occupation, number of children and
marital status.

The prediction of individuals' wages has been an issue in previous
literature. As wages are relevant to companies they want to know how
predict wages \autocite{Chakraborti}. Some papers make use of
classification techniques to predict wages \autocite{Chakraborti}. As we
have numeric values of real wages and not a categorical variable we do
not make use of classification techniques but prefer linear regression
techniques such as OLS and Lasso regressions as well as random forests.
We prefer these techniques as they handle categorical variables and
outliers well, which are features of our data \autocite{Cutler}. Wages
were already predicted using random forests by \autocite{Eichinger}.
They find that using random forests results in better predictions than
using linear models. Additionally, they show that gender barely effects
the wage predictions. Likewise, the boosting methods used by
\autocite{Chakrabarty2018} in salary level classification problems
showed better performance compared to a traditional algorithm such as
SVM. Furthermore, \autocite{BonaccoltoToepfer1612} suggests the use of a
penalization approach for high dimensional problems in his research
about Gender wage inequality.

Based on the previous literature, we use several regression techniques
to predict \texttt{realrinc}:

\begin{itemize}
\item
  Linear Regression
\item
  Logistic Regression
\item
  Lasso, Ridge and Elastic Net Regression
\item
  Gradient Boosting
\item
  Random Forest
\end{itemize}

In order to evaluate which of the regression techniques predicts
\texttt{realrinc} the best we split the data set into a train (70\%) and
a test (30\%) data set. We fit the aforementioned regression models on
the train data and evaluate the root mean squared error (RMSE) using the
validation data. We choose the RMSE as it is more sensitive to large
errors \autocite{Arour}.

Furthermore, we use cross validation (CV) to assess the performance of a
model and to select appropriate hyperparameters. CV prevents overfitting
and helps in model selection.

\hypertarget{data-preparation-feature-engineering}{%
\section{Data Preparation \& Feature
Engineering}\label{data-preparation-feature-engineering}}

The initial data set contains 11 variables, which are described in the
following:

\begin{itemize}
\item
  \textbf{year}: survey year
\item
  \textbf{realrinc}: respondent's base income (in constant 1986 USD)
\item
  \textbf{age}: respondent's age in years
\item
  \textbf{occ10}: respondent's occupation code
\item
  \textbf{occrecode}: recode of the occupation code into one of 11 main
  categories
\item
  \textbf{prestg10}: respondent's occupational prestige score
\item
  \textbf{childs}: number of children
\item
  \textbf{wrkstat}: work status of the respondent (full-time, part-time,
  temporarily not working, unemployed (laid off), etc.)
\item
  \textbf{gender}: respondent's gender
\item
  \textbf{educcat}: respondent's degree level (Less Than High School,
  High School, etc.)
\item
  \textbf{maritalcat}: respondent's marital status (Married, Widowed,
  Divorced, etc.)
\end{itemize}

\hypertarget{data-preparation}{%
\subsection{Data Preparation}\label{data-preparation}}

Before modeling we conduct a data preparation process. The initial data
set has 54,842 observations and the outcome variable \texttt{realrinc}
has 38.55\% of missing values. The imputation of the target variable is
discarded to avoid generating a bias in the modeling and therefore we
decided to eliminate these records from the original data set. After
this, the dataset contains 33,702 observations. Furthermore, it is
impossible to deduce the occupation for some observations
(\texttt{occ10\ =\ 9997}) and we remove these from the data set. At the
end, the final data set has 33,244 observations, and we use it for the
modeling process.

This data set still had some missing values for the regressors, e.g.,
number of children, age, marital status, degree level, and others. We
use the \texttt{mice} package with five variables for the imputation.
The features \emph{age, occupational prestige and number of children}
are imputed using the pmm (\texttt{predictive\ mean\ matching}) method.
We use the polytomous logistic regression method for the factor
variables (\emph{education and marital status}). The imputation process
generates 6 different data sets. We apply the estimation of the mean and
the majority vote to aggregate the data sets into a single data set. The
mean estimation is applied for the numeric variables and majority vote
for the categorical variables..

\hypertarget{feature-engineering}{%
\subsection{Feature Engineering}\label{feature-engineering}}

After the imputation process, we create the interaction variables.
First, we estimate some interaction features using only the numerical
variables. Then, we transform the numerical variables into categorical
variables by dividing them into different ranges (e.g., age between 18
and 30, age between 31 and 50, etc.). We transform these new categorical
variables and the initial categorical variables (\emph{marital status,
education, etc.}) into dummy variables. And finally, the interaction
variables are estimated with the group of dummy variables mentioned
above. The inclusion of the interaction of the numeric variables are
validate using a linear model, and the results show that the p- values
of the parameters for the interactions are less than 0.05.

\begin{figure}
\centering
\includegraphics[width=3.30208in,height=\textheight]{includes/reg_table.png}
\caption{Linear Model: Inclusion of Interaction Variables}
\end{figure}

Furthermore, we estimate a correlation matrix using Spearman's rank
correlation coefficient with some numeric variables in order to support
the previous results. The correlation results are shown in the following
figure.

\begin{figure}
\centering
\includegraphics{includes/corr_matrix.png}
\caption{Correlation Matrix}
\end{figure}

After the data preparation and feature engineering processes, we
construct a final data set with 33,244 observations and 421 features.
Due to the large number of interaction variables, we decided to reduce
them and use only the interaction between a group of variables
(\emph{occrecode, educcat and prestg10}). After reduction, we develop
the modeling process with 100 variables (numerical and interaction
variables). The following figure shows the overview of the group of
variables that were estimated and used in the models.

\begin{figure}
\centering
\includegraphics{includes/feature_process.png}
\caption{Feature Engineering}
\end{figure}

\hypertarget{model-training-and-tuning}{%
\section{Model Training and Tuning}\label{model-training-and-tuning}}

We use five different regression techniques to predict the individuals'
wages using the final data set. We fit every model on our training data
using the \texttt{train} function from the \texttt{caret} package. To
choose the best parameter combination we use cross-validation within all
train functions.

\hypertarget{linear-and-logistic-regression}{%
\subsection{Linear and Logistic
Regression}\label{linear-and-logistic-regression}}

Our baseline linear model uses an OLS regression including all
interaction variables. This results in an RMSE of 25,506.05.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{formula }\OtherTok{\textless{}{-}} \FunctionTok{as.formula}\NormalTok{(}\FunctionTok{paste}\NormalTok{(}\StringTok{"realrinc \textasciitilde{}"}\NormalTok{, }
                            \FunctionTok{paste}\NormalTok{(x\_cols\_dummys, }\AttributeTok{collapse =} \StringTok{" + "}\NormalTok{)))}
\NormalTok{mod\_full }\OtherTok{\textless{}{-}} \FunctionTok{train}\NormalTok{(formula,}
                  \AttributeTok{data =}\NormalTok{ train, }
                  \AttributeTok{method =} \StringTok{"lm"}\NormalTok{,  }
                  \AttributeTok{trControl =} \FunctionTok{trainControl}\NormalTok{(}\AttributeTok{method =} \StringTok{"cv"}\NormalTok{, }\AttributeTok{number =} \DecValTok{3}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Given some extreme outliers in the income distribution of our data
(which is typical for wage data) we follow the economic literature
\autocite{Ermini} and fit a logistic model to our data, using the log of
\texttt{realrinc} as our dependent variable. This results in a higher
RMSE than the linear model (25,909.82). Still, the linear model yields
predictions of \texttt{realrinc} that are negative, what is implausible.
Using the log of \texttt{realrinc} only positive predictions are
generated. Therefore, we prefer using the log for the following
predictions.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{formula2 }\OtherTok{\textless{}{-}} \FunctionTok{as.formula}\NormalTok{(}\FunctionTok{paste}\NormalTok{(}\StringTok{"log\_realrinc \textasciitilde{}"}\NormalTok{, }
                              \FunctionTok{paste}\NormalTok{(x\_cols\_dummys, }\AttributeTok{collapse =} \StringTok{" + "}\NormalTok{)))}
\NormalTok{mod\_loginc }\OtherTok{\textless{}{-}} \FunctionTok{train}\NormalTok{(formula2,}
                    \AttributeTok{data =}\NormalTok{ train, }
                    \AttributeTok{method =} \StringTok{"lm"}\NormalTok{,  }
                    \AttributeTok{trControl =} \FunctionTok{trainControl}\NormalTok{(}\AttributeTok{method =} \StringTok{"cv"}\NormalTok{, }\AttributeTok{number =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

In both models the coefficient of female is statistically significant
with p \textless{} 0.001. It suggests that being female decreases the
real income by approximately 10,192\$ in the linear model or by 40\% in
the logistic model.

\hypertarget{subset-selection}{%
\subsection{Subset Selection}\label{subset-selection}}

As the dataset contains many variables (361) and even our selection of
dummy variables for the right-hand side of the regression leaves us with
53 variables, we make use of regression techniques for subset selection.
We use lasso, ridge and elastic net regressions. These methods allow us
to fit a model that contains all variables. Then the model regularizes
the coefficient estimates and shrinks them towards zero. In effect, we
reduce the estimates' variance which improves the model fit.

We start with a ridge regression.

\[
y = RSS + \lambda \sum^p _{j=1} \beta^2_j 
\]

Hence, the coefficient estimates in a ridge regression minimize the sum
of the residual sum of squares (RSS) and the sum of squared coefficients
multiplied with a tuning parameter \(\lambda\). We estimate \(\lambda\)
using cross-validation.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ridge }\OtherTok{\textless{}{-}} \FunctionTok{train}\NormalTok{(formula, }\AttributeTok{data =}\NormalTok{ train,}
  \AttributeTok{method =} \StringTok{"glmnet"}\NormalTok{, }\AttributeTok{trControl =} \FunctionTok{trainControl}\NormalTok{(}\AttributeTok{method =} \StringTok{"cv"}\NormalTok{, }\AttributeTok{number =} \DecValTok{3}\NormalTok{),}
  \AttributeTok{tuneGrid =} \FunctionTok{expand.grid}\NormalTok{(}\AttributeTok{alpha =} \DecValTok{0}\NormalTok{, }\AttributeTok{lambda =} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\FloatTok{0.001}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

Performing a ridge regression to predict real incomes yields an RMSE of
26,062.58.

In contrast to other subset selection methods ridge regression does not
exclude any variables from the regression model. Hence, the model is
still fitted on 53 covariates. To overcome this issue we also use a
lasso regression.

\[
RSS + \lambda \sum^p_{j=1} | \beta_j |
\]

In contrast to the ridge regression the lasso regression forces some of
the coefficient estimates to be equal to zero if \(\lambda\) is
sufficiently large. Hence, the regression is not performed on all
covariates.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lasso }\OtherTok{\textless{}{-}} \FunctionTok{train}\NormalTok{(formula, }\AttributeTok{data =}\NormalTok{ train, }
       \AttributeTok{method =} \StringTok{"glmnet"}\NormalTok{, }\AttributeTok{trControl =} \FunctionTok{trainControl}\NormalTok{(}\AttributeTok{method =} \StringTok{"cv"}\NormalTok{), }
       \AttributeTok{tuneGrid =} \FunctionTok{expand.grid}\NormalTok{(}\AttributeTok{alpha =} \DecValTok{1}\NormalTok{, }\AttributeTok{lambda =} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\FloatTok{0.001}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

To perform the lasso regression in \texttt{R} we set \(\alpha\) equal to
one. The lasso regression yields a lower RMSE than the ridge regression
(25,918.25 vs.~26,062.58).

In a final step we use elastic net regularization which is a linear
combination of ridge and lasso regression. The elastic net
regularization allows \(\alpha\) to vary between zero and one. This
results in an RMSE of 25,918.81.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{elasticnet }\OtherTok{\textless{}{-}} \FunctionTok{train}\NormalTok{(formula, }\AttributeTok{data =}\NormalTok{ train,}
       \AttributeTok{method =} \StringTok{"glmnet"}\NormalTok{, }\AttributeTok{trControl =} \FunctionTok{trainControl}\NormalTok{(}\AttributeTok{method =} \StringTok{"cv"}\NormalTok{),}
       \AttributeTok{tuneGrid =} \FunctionTok{expand.grid}\NormalTok{(}\AttributeTok{alpha =} \FunctionTok{seq}\NormalTok{(}\AttributeTok{from=}\DecValTok{0}\NormalTok{, }\AttributeTok{to=}\DecValTok{1}\NormalTok{, }\AttributeTok{by =} \FloatTok{0.1}\NormalTok{),}
       \AttributeTok{lambda =} \FunctionTok{seq}\NormalTok{(}\AttributeTok{from=}\DecValTok{0}\NormalTok{, }\AttributeTok{to=}\FloatTok{0.15}\NormalTok{, }\AttributeTok{by =} \FloatTok{0.001}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\hypertarget{random-forest}{%
\subsection{Random Forest}\label{random-forest}}

According to the literature review, tree-based models perform
considerably well in range in predicting income ranges (classification
problems). For this reason, we use the random forest algorithm for
modeling. This experiment use the income without logarithm
transformation, the initial features and some interaction variables.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tnr\_rdgrid\_search }\OtherTok{=} \FunctionTok{tnr}\NormalTok{(}\StringTok{"random\_search"}\NormalTok{, }\AttributeTok{batch\_size =} \DecValTok{10}\NormalTok{)}
\NormalTok{rsmp\_cv3 }\OtherTok{=} \FunctionTok{rsmp}\NormalTok{(}\StringTok{"cv"}\NormalTok{, }\AttributeTok{folds =} \DecValTok{3}\NormalTok{)}
\NormalTok{msr\_ce }\OtherTok{=} \FunctionTok{msr}\NormalTok{(}\StringTok{"regr.rmse"}\NormalTok{)}
\NormalTok{learner }\OtherTok{=} \FunctionTok{lrn}\NormalTok{(}\StringTok{"regr.lightgbm"}\NormalTok{,}
              \AttributeTok{boosting =} \StringTok{"rf"}\NormalTok{,}
              \AttributeTok{objective =} \StringTok{"regression"}\NormalTok{,}
              \AttributeTok{max\_depth =} \FunctionTok{to\_tune}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\DecValTok{5}\NormalTok{,}\DecValTok{12}\NormalTok{,}\DecValTok{1}\NormalTok{)),}
              \AttributeTok{num\_leaves =} \FunctionTok{to\_tune}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\DecValTok{270}\NormalTok{,}\DecValTok{280}\NormalTok{,}\DecValTok{1}\NormalTok{)),}
              \AttributeTok{num\_iterations  =} \FunctionTok{to\_tune}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\DecValTok{132}\NormalTok{,}\DecValTok{137}\NormalTok{,}\DecValTok{1}\NormalTok{)),}
              \AttributeTok{min\_data\_in\_leaf =} \FunctionTok{to\_tune}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\DecValTok{70}\NormalTok{,}\DecValTok{85}\NormalTok{,}\DecValTok{1}\NormalTok{)),}
              \AttributeTok{min\_data\_in\_bin =} \FunctionTok{to\_tune}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\DecValTok{10}\NormalTok{,}\DecValTok{15}\NormalTok{, }\DecValTok{1}\NormalTok{)),}
              \AttributeTok{feature\_fraction\_bynode =} \FunctionTok{to\_tune}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\FloatTok{0.3}\NormalTok{,}\FloatTok{0.4}\NormalTok{,}\FloatTok{0.1}\NormalTok{)),}
              \AttributeTok{bagging\_fraction =} \FunctionTok{to\_tune}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\FloatTok{0.2}\NormalTok{,}\FloatTok{0.3}\NormalTok{,}\FloatTok{0.1}\NormalTok{)),}
              \AttributeTok{bagging\_freq =} \FunctionTok{to\_tune}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\DecValTok{3}\NormalTok{,}\DecValTok{5}\NormalTok{,}\DecValTok{1}\NormalTok{)),}
              \AttributeTok{feature\_fraction =} \FunctionTok{to\_tune}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\FloatTok{0.7}\NormalTok{,}\FloatTok{0.9}\NormalTok{,}\FloatTok{0.1}\NormalTok{)),}
              \AttributeTok{convert\_categorical =} \ConstantTok{TRUE}\NormalTok{,}
              \AttributeTok{force\_col\_wise =} \ConstantTok{TRUE}\NormalTok{,}
              \AttributeTok{verbose =} \DecValTok{1}\NormalTok{,}
              \AttributeTok{num\_threads =} \DecValTok{5}
\NormalTok{)}

\NormalTok{instance.rf }\OtherTok{=} \FunctionTok{tune}\NormalTok{(}
  \AttributeTok{tuner =}\NormalTok{ tnr\_rdgrid\_search,}
  \AttributeTok{task =}\NormalTok{ task,}
  \AttributeTok{learner =}\NormalTok{ learner,}
  \AttributeTok{resampling =}\NormalTok{ rsmp\_cv3,}
  \AttributeTok{measures =}\NormalTok{ msr\_ce,}
  \AttributeTok{term\_evals =} \DecValTok{150}\NormalTok{,}
  \AttributeTok{store\_models =} \ConstantTok{FALSE}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Due to the number of hyper parameters in random forest, we use the
\texttt{lightgbm} and \texttt{mlr3} packages to model and tune the hyper
parameter in a time-optimal way. After many attempts, we found the best
hyper parameters that produced the lowest error. The hyper parameters
tuned were \texttt{max\_depth}, \texttt{num\_leaves},
\texttt{num\_iterations} and others. In addition, we used a random
search approach in the tuning process with a cross-validation of 3. Once
we found the model along with the hyper parameters with lowest error in
the cross-validation, we proceeded to evaluate this model on the test
data set and obtained a RMSE of 31,745.54. The hyper parameters of the
best model are shown in the following table.

{[}best hyper parameter of RF{]}

In addition, we can analyze the importance of the features within the
modeling process using random forest, and these results are shown in the
following table.

{[}feature importance of RF{]}

\hypertarget{gradient-boosting}{%
\subsection{Gradient Boosting}\label{gradient-boosting}}

Gradient Boosting also has many hyper parameters like random forest and
we decided to apply a similar approach to the previous algorithm to find
the best model with the hyper parameters (random search approach in the
tuning process with a cross-validation of 3). Furthermore, the
experiments with Gradient Boosting use the target variable without
logarithm transformation.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tnr\_rdgrid\_search }\OtherTok{=} \FunctionTok{tnr}\NormalTok{(}\StringTok{"random\_search"}\NormalTok{, }\AttributeTok{batch\_size =} \DecValTok{10}\NormalTok{)}
\NormalTok{rsmp\_cv3 }\OtherTok{=} \FunctionTok{rsmp}\NormalTok{(}\StringTok{"cv"}\NormalTok{, }\AttributeTok{folds =} \DecValTok{3}\NormalTok{)}
\NormalTok{msr\_ce }\OtherTok{=} \FunctionTok{msr}\NormalTok{(}\StringTok{"regr.rmse"}\NormalTok{)}
\NormalTok{learner.gb }\OtherTok{=} \FunctionTok{lrn}\NormalTok{(}\StringTok{"regr.lightgbm"}\NormalTok{,}
                 \AttributeTok{boosting =} \StringTok{"gbdt"}\NormalTok{,}
                 \AttributeTok{objective =} \StringTok{"regression"}\NormalTok{,}
                 \AttributeTok{max\_depth =} \FunctionTok{to\_tune}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{1}\NormalTok{)),}
                 \AttributeTok{num\_leaves =} \FunctionTok{to\_tune}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\DecValTok{6}\NormalTok{,}\DecValTok{8}\NormalTok{,}\DecValTok{1}\NormalTok{)),}
                 \AttributeTok{min\_data\_in\_leaf =} \FunctionTok{to\_tune}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\DecValTok{39}\NormalTok{,}\DecValTok{45}\NormalTok{,}\DecValTok{1}\NormalTok{)),}
                 \AttributeTok{min\_data\_in\_bin =} \FunctionTok{to\_tune}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\DecValTok{8}\NormalTok{,}\DecValTok{11}\NormalTok{,}\DecValTok{1}\NormalTok{)),}
                 \AttributeTok{feature\_fraction =} \FunctionTok{to\_tune}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\FloatTok{0.3}\NormalTok{,}\FloatTok{0.5}\NormalTok{,}\FloatTok{0.1}\NormalTok{)),}
                 \AttributeTok{feature\_fraction\_bynode =} \FunctionTok{to\_tune}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\FloatTok{0.1}\NormalTok{,}\FloatTok{0.2}\NormalTok{,}\FloatTok{0.1}\NormalTok{)),}
                 \AttributeTok{learning\_rate =} \FunctionTok{to\_tune}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\FloatTok{0.03}\NormalTok{, }\FloatTok{0.04}\NormalTok{, }\FloatTok{0.01}\NormalTok{)),}
                 \AttributeTok{num\_iterations  =} \FunctionTok{to\_tune}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\DecValTok{39}\NormalTok{,}\DecValTok{45}\NormalTok{,}\DecValTok{1}\NormalTok{)), }\CommentTok{\#40}
                 \AttributeTok{lambda\_l1 =} \FunctionTok{to\_tune}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\FloatTok{0.4}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\FloatTok{0.1}\NormalTok{)),}
                 \AttributeTok{lambda\_l2 =} \FunctionTok{to\_tune}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\FloatTok{0.30}\NormalTok{, }\FloatTok{0.33}\NormalTok{, }\FloatTok{0.01}\NormalTok{)),}
                 \AttributeTok{convert\_categorical =} \ConstantTok{TRUE}\NormalTok{,}
                 \AttributeTok{force\_col\_wise =} \ConstantTok{TRUE}\NormalTok{,}
                 \AttributeTok{verbose =} \DecValTok{1}\NormalTok{,}
                 \AttributeTok{num\_threads =} \DecValTok{5}
\NormalTok{)}

\NormalTok{instance.gb }\OtherTok{=} \FunctionTok{tune}\NormalTok{(}
  \AttributeTok{tuner =}\NormalTok{ tnr\_rdgrid\_search,}
  \AttributeTok{task =}\NormalTok{ task,}
  \AttributeTok{learner =}\NormalTok{ learner.gb,}
  \AttributeTok{resampling =}\NormalTok{ rsmp\_cv3,}
  \AttributeTok{measures =}\NormalTok{ msr\_ce,}
  \AttributeTok{term\_evals =} \DecValTok{170}\NormalTok{,}
  \AttributeTok{store\_models =} \ConstantTok{FALSE}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We tried many experiments and found the best hyper parameters that
produced the lowest error. In Gradient Boosting modeling we adjusted the
hyper parameters such as \texttt{max\_depth}, \texttt{num\_leaves},
\texttt{min\_data\_in\_leaf}, \texttt{feature\_fraction} and others.
Once the best model was found, it obtained an RMSE of 29,207.55 on the
test dataset. The hyper parameters of the best model are shown in the
following table.

{[}best hyper parameter of GB{]}

Furthermore, the following table shows the importance of the features
within the modeling process using Gradient Boosting.

{[}feature importance of GB{]}

These results show that in tree-based methods, the Gradient Boosting
performance overcomes Random Forest, in other words, the error of
Gradient Boosting is approximately 8\% lower than the error of Random
Forest. In addition, the errors of these models are larger than the
linear models.

\hypertarget{evaluation}{%
\section{Evaluation}\label{evaluation}}

The aim of this paper is to predict individuals' wages. In order to
evaluate the best prediction method we use the RMSE. The following table
demonstrates the RMSE for each used method: \medskip

\begin{tabular}{l c} 
\textbf{Method} & \textbf{RMSE} \\ 
Linear Regression & 25,506.05 \\ 
Logistic Regression & 25,909.82 \\ 
Ridge & 26,062.58 \\ 
Lasso & 25,918.25 \\
Elastic Net & 25,918.81 \\ 
Random Forest & 31,745.54 \\ 
Gradient Boosting & 29,207.55 \\ 
\end{tabular}

\medskip

Conclusively, the linear regression yields the lowest RMSE and thus
appears to be the best prediction method. Still, it delivers negative
predictions of the wage. Therefore, we prefer the logistic regression as
the best prediction method.

\end{document}